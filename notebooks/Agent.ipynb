{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Implementation Documentation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements an AI agent that combines natural language processing capabilities with web search functionality. The implementation leverages:\n",
    "\n",
    "- LangGraph for agent orchestration and state management\n",
    "- LangChain for LLM integration and tool management\n",
    "- Tavily API for web search capabilities\n",
    "- OpenAI's model for natural language understanding\n",
    "\n",
    "The agent maintains conversation state using memory checkpoints and can perform web searches to gather information in response to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "\n",
    "The notebook uses environment variables for secure configuration management. The `load_dotenv()` function loads API keys and other sensitive configuration from a `.env` file. This approach:\n",
    "\n",
    "- Ensures secure handling of API keys and credentials\n",
    "- Maintains separation between configuration and code\n",
    "- Facilitates deployment across different environments\n",
    "\n",
    "The `override=True` parameter ensures that existing environment variables take precedence over those in the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "  \n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Initialization\n",
    "\n",
    "This section initializes the core components of the AI agent:\n",
    "\n",
    "### Key Components\n",
    "- `MemorySaver`: Manages conversation state and history\n",
    "- `TavilySearchResults`: Implements web search functionality\n",
    "- `create_react_agent`: Creates an agent that can reason and act\n",
    "\n",
    "### Implementation Details\n",
    "- The agent uses a memory checkpoint system for state persistence\n",
    "- Web search is configured to return a maximum of 2 results per query\n",
    "- The agent is initialized with a single tool (web search)\n",
    "\n",
    "### Best Practices\n",
    "- Tools are modular and can be extended with additional functionality\n",
    "- Memory management enables context-aware conversations\n",
    "- The agent architecture follows the ReAct (Reasoning and Acting) pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant functionality\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Create the agent\n",
    "memory = MemorySaver()\n",
    "model = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "search = TavilySearchResults(max_results=2)\n",
    "tools = [search]\n",
    "agent_executor = create_react_agent(model, tools, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Interaction Example\n",
    "\n",
    "This section demonstrates basic agent interaction capabilities:\n",
    "\n",
    "### Implementation Details\n",
    "- Uses a unique thread ID for conversation tracking\n",
    "- Implements streaming mode for real-time response generation\n",
    "- Formats messages with clear human/agent distinction\n",
    "\n",
    "### Key Features\n",
    "- Natural language understanding and response generation\n",
    "- Context-aware conversation management\n",
    "- Structured message formatting for clear communication\n",
    "\n",
    "### Technical Notes\n",
    "- The `stream_mode=\"values\"` parameter enables real-time message streaming\n",
    "- Message formatting includes clear visual separation between human and agent messages\n",
    "- The thread ID system enables multiple concurrent conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi im bob! and i live in sf\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! It's great to meet you. How's life in San Francisco?\n"
     ]
    }
   ],
   "source": [
    "# Use the agent\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"hi im bob! and i live in sf\")]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Interaction with Web Search\n",
    "\n",
    "This section demonstrates the agent's ability to combine natural language processing with web search:\n",
    "\n",
    "### Implementation Details\n",
    "- Uses the Tavily search tool to gather real-time information\n",
    "- Implements token-by-token streaming for real-time response generation\n",
    "- Filters messages to show only agent responses\n",
    "\n",
    "### Key Features\n",
    "- Dynamic information retrieval from the web\n",
    "- Real-time response generation with token streaming\n",
    "- Context-aware responses based on previous conversation\n",
    "\n",
    "### Technical Notes\n",
    "- The `stream_mode=\"messages\"` parameter enables fine-grained control over message streaming\n",
    "- Message filtering ensures only relevant agent responses are displayed\n",
    "- The agent maintains context from previous interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The| current| weather| in| San| Francisco| is| cloudy| with| over|cast| skies|.| The| daytime| temperature| is| about| |66|°F|,| and| it| will| cool| down| to| around| |54|°F| at| night|.| \n",
      "\n",
      "|If| you|’d| like| more| information|,| you| can| check| out| [|this| link|](|https|://|we|athers|hog|un|.com|/weather|/|usa|/|ca|/s|an|-fr|anc|isco|/|480|/ap|ril|/|202|5|-|04|-|16|).|"
     ]
    }
   ],
   "source": [
    "# Stream tokens\n",
    "for step, metadata in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"whats the weather where I live?\")]},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if metadata[\"langgraph_node\"] == \"agent\" and (text := step.text()):\n",
    "        print(text, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
